# kg-agent 下一步实施设计与工作计划

## 1. 总体目标
将当前的 Mock（模拟）实现替换为真实可用的业务逻辑，构建完整的文档处理与索引流水线（RAG 基础架构）。

## 2. 核心模块概要设计

### 2.1 文档处理流水线 (ETL)
利用 `LangChain` 框架实现文档的加载、清洗与分块。

*   **文本提取 (Extract)**:
    *   **技术选型**: `langchain_community.document_loaders`
    *   **支持格式**:
        *   `.txt`: `TextLoader`
        *   `.pdf`: `PyPDFLoader`
        *   `.docx`: `UnstructuredWordDocumentLoader` (视依赖情况，或者使用基础库)
    *   **实现逻辑**: 根据文件扩展名选择对应的 Loader 加载为 LangChain `Document` 对象。

*   **文本分块 (Transform)**:
    *   **技术选型**: `langchain.text_splitter.RecursiveCharacterTextSplitter`
    *   **策略**:
        *   Chunk Size: 500-1000 字符 (根据 Embedding 模型上下文调整)
        *   Overlap: 100-200 字符 (保持上下文连贯性)
    *   **输出**: 标准化的 Chunk 列表，包含内容和元数据（来源文件、位置信息）。

*   **向量化 (Embedding)**:
    *   **技术选型**: `sentence-transformers` (本地运行) 或 OpenAI API (可选)
    *   **模型**: `all-MiniLM-L6-v2` (轻量高效，适合 CPU/开发环境) 或 `m3e-base` (中文优化)。
    *   **实现**: 将 Chunk 文本转换为 float 向量列表。

### 2.2 存储层集成 (Load)
实现数据向多模态数据库的写入。

*   **向量存储 (Milvus)**:
    *   **Schema 设计**:
        *   `id`: Int64 (Auto ID)
        *   `doc_id`: Varchar (关联源文档)
        *   `chunk_id`: Varchar
        *   `content`: Varchar (可选，视存储策略定)
        *   `embedding`: FloatVector (Dim: 384/768)
    *   **操作**: 创建 Collection -> 创建 Index (IVF_FLAT/HNSW) -> 插入数据。

*   **全文检索 (Elasticsearch)**:
    *   **Index 设计**: `kg_documents`
    *   **Mapping**:
        *   `doc_id`: keyword
        *   `content`: text (analyzer: ik_max_word for Chinese)
        *   `metadata`: object
    *   **操作**: 幂等写入，支持更新。

*   **图数据库 (NebulaGraph)**:
    *   **Schema 设计**:
        *   **Tag**: `Document` (name, type), `Chunk` (index), `Entity` (name, type)
        *   **EdgeType**: `HAS_CHUNK` (Document -> Chunk), `MENTIONS` (Chunk -> Entity), `RELATED_TO` (Entity -> Entity)
    *   **初步实现**: 
        *   构建 Document -> Chunk 的基础图结构。
        *   (进阶) 简单的关键词/实体提取作为图节点（暂不引入复杂的 LLM 抽取，先跑通流程）。

## 3. 详细工作计划

### 阶段一：真实文档处理 (预计 1-2 步)
1.  **依赖完善**: 确认 LangChain 及相关解析库 (`pypdf`, `docx` 等) 已安装。
2.  **Loader 实现**: 在 `app/utils/text_processor.py` 中封装通用的 `load_document` 函数。
3.  **Splitter 实现**: 封装 `split_document` 函数，配置合理的分块参数。
4.  **Task 更新**: 修改 `tasks/document.py`，替换 Mock 逻辑调用上述真实函数。

### 阶段二：向量化与 Milvus 集成 (预计 2-3 步)
1.  **Embedding 服务**: 封装 `EmbeddingService`，单例加载模型（避免每次任务重复加载）。
2.  **Milvus 连接**: 在 `app/infrastructure/milvus.py` 实现连接池和 Collection 初始化逻辑。
3.  **写入逻辑**: 在 `tasks/index.py` 中实现向量生成与 Milvus 插入。

### 阶段三：ES 与 Nebula 集成 (预计 2-3 步)
1.  **ES 客户端**: 配置 `Elasticsearch` 连接，定义索引 Template。
2.  **Nebula 客户端**: 配置 `NebulaGraph` 连接池 (Session Pool)。
3.  **图谱构建**: 实现基础的 Document-Chunk 图谱写入逻辑。
4.  **综合测试**: 上传真实 PDF/TXT 文件，验证三个数据库的数据一致性。

## 4. 待确认事项
*   **Embedding 模型选择**: 是否直接使用本地 CPU 友好的 `all-MiniLM-L6-v2`？
*   **中文分词**: ES 是否已配置 IK 分词器？如果没有，先使用标准分词或 ngram。
*   **图谱深度**: 当前阶段是否仅建立结构化图谱（文档-分块），还是需要引入简单的实体抽取？建议先做结构化图谱，确保流程跑通。
