# 知识图谱全文检索单体服务改造方案

## 1. 概述

本文档旨在将原定的基于微服务架构的“知识图谱全文检索系统”改造为适合单台服务器部署的**单体服务架构**。改造的核心目标是降低运维复杂度、减少资源消耗，同时保持原方案中关于 GraphRAG（图谱增强检索）的核心技术实现不变。

## 2. 架构改造分析

### 2.1 原微服务架构梳理

原方案采用典型的微服务分层架构：
*   **应用服务层**：包含 SearchAPI、KGAPI、AdminAPI 三个独立服务，通过 API Gateway 路由。
*   **知识处理层**：包含 DocIngest、NLPProcess、KGBuilder、VectorEncoder 四个独立 Worker，通过 Redis 消息队列异步通信。
*   **数据存储层**：依赖 MinIO、Elasticsearch、Milvus、NebulaGraph、Redis 等多个分布式组件。

**痛点分析（针对单机部署）**：
1.  **组件过多**：运维管理复杂，Docker 容器编排繁琐。
2.  **通信开销**：服务间 HTTP/RPC 调用和序列化带来不必要的延迟。
3.  **资源浪费**：每个微服务独立的运行环境（Python 解释器、Web Server 进程）占用大量内存。

### 2.2 单体架构设计理念

*   **进程内通信替代网络通信**：将原来的 HTTP 调用改为 Python 模块间的函数调用。
*   **共享运行时**：所有业务逻辑运行在同一个 Flask 应用进程或 Celery Worker 进程中。
*   **简化中间件**：移除 API Gateway，统一由 Nginx 做反向代理；简化存储组件部署方式。

## 3. 单体服务架构设计

### 3.1 系统架构图

```mermaid
graph TD
    User[用户] --> WebUI[React 前端应用]
    WebUI --> Nginx[Nginx 反向代理]
    
    subgraph "单体后端服务 (Monolithic Backend)"
        Nginx --> FlaskApp[Flask 主应用进程]
        
        subgraph "模块划分 (Python Modules)"
            FlaskApp --> APIBlueprint[API 蓝图 (Search/KG/Admin)]
            APIBlueprint --> ServiceLayer[业务服务层]
            
            ServiceLayer --> SearchEngine[检索引擎模块]
            ServiceLayer --> KGEngine[图谱引擎模块]
            ServiceLayer --> DocManager[文档管理模块]
            
            SearchEngine --> RAGCore[RAG 核心逻辑]
        end
        
        ServiceLayer -.-> AsyncTasks[异步任务队列 (Celery)]
    end
    
    subgraph "异步处理进程 (Celery Worker)"
        AsyncTasks --> TaskWorker[通用任务 Worker]
        TaskWorker --> Pipeline[处理流水线]
        
        Pipeline --> P_Extract[文本提取]
        Pipeline --> P_Vector[向量编码]
        Pipeline --> P_NLP[NLP 处理]
        Pipeline --> P_Graph[图谱构建]
    end
    
    subgraph "本地数据存储 (Docker Compose)"
        FlaskApp & TaskWorker --> LocalFS[本地文件存储 (替代 MinIO)]
        FlaskApp & TaskWorker --> ES_Single[Elasticsearch (单节点)]
        FlaskApp & TaskWorker --> Milvus_Lite[Milvus Lite / FAISS]
        FlaskApp & TaskWorker --> Nebula_Single[NebulaGraph (单机版)]
        FlaskApp & TaskWorker --> Redis[Redis (缓存+队列)]
        FlaskApp & TaskWorker --> SQLite[SQLite/MySQL (元数据)]
    end
```

### 3.2 模块合并策略

1.  **API 层合并**：
    *   取消独立的 SearchAPI、KGAPI、AdminAPI 服务。
    *   使用 Flask Blueprint 将它们合并为同一个 Flask 应用的不同路由模块：
        *   `/api/search/*` -> `search_blueprint`
        *   `/api/kg/*` -> `kg_blueprint`
        *   `/api/admin/*` -> `admin_blueprint`

2.  **业务逻辑层合并**：
    *   原微服务中的业务逻辑代码重构为 Python Package 下的不同模块（`services.search`, `services.kg`, `services.ingest`）。
    *   模块间调用直接通过 `import` 和函数调用实现，移除 HTTP 请求代码。

3.  **异步任务合并**：
    *   原 DocIngest、NLPProcess 等独立 Worker 合并为一个通用的 Celery Worker 服务。
    *   通过 Celery 的 Task 路由机制区分不同类型的任务，但运行在同一个进程池中，共享内存资源（如加载一次 NLP 模型）。

## 4. 基础设施与存储适配

针对单机环境，对基础设施进行轻量化调整：

### 4.1 存储组件调整

| 组件 | 原方案 | 单体/单机方案 | 改造说明 |
| :--- | :--- | :--- | :--- |
| **文件存储** | MinIO Cluster | **Local Filesystem / MinIO Single** | 优先使用本地文件系统存储上传文档，或部署单节点 MinIO 以保持接口一致性。 |
| **向量库** | Milvus Cluster | **Milvus Lite / FAISS** | 对于中小规模数据（<100万向量），使用嵌入式的 Milvus Lite 或直接集成 FAISS 库，无需独立部署服务。 |
| **全文检索** | Elasticsearch Cluster | **Elasticsearch Single Node** | 部署单节点 ES，限制堆内存（如 2GB），关闭副本分片。 |
| **图数据库** | NebulaGraph Cluster | **NebulaGraph Docker (Standalone)** | 使用 NebulaGraph 的单机 Docker 镜像，包含 Graphd/Metad/Storaged 所有服务。 |
| **元数据** | PostgreSQL/MySQL | **SQLite / PostgreSQL Docker** | 推荐使用 SQLite 减少维护成本，或单容器 PostgreSQL。 |

### 4.2 服务发现与网关

*   **移除 API Gateway**：不再需要 Kong 或 Spring Cloud Gateway。
*   **Nginx 代理**：在单机上部署一个 Nginx，负责：
    *   托管 React 前端静态资源。
    *   反向代理 `/api` 请求到本地 Flask 端口（如 5000）。
    *   SSL 终结（如果需要）。

## 5. 实施步骤

### 5.1 阶段一：代码重构 (Refactoring)

1.  **建立单体工程结构**：
    ```text
    /src
      /backend
        /app
          /api          # 存放所有 Blueprint (Search, KG, Admin)
          /services     # 核心业务逻辑 (原微服务核心代码)
          /models       # 数据库模型
          /tasks        # Celery 任务定义
          /utils        # 工具类
        config.py       # 统一配置文件
        main.py         # Flask 入口
    ```
2.  **合并 API 接口**：将原分散的 Swagger/OpenAPI 定义合并，统一注册到 Flask App。
3.  **内联服务调用**：查找所有 `requests.post('http://other-service/...')` 代码，替换为 `from services.other import func; func(...)`。

### 5.2 阶段二：存储层适配 (Storage Adaptation)

1.  **本地化文件适配器**：编写 `FileManager` 接口，支持切换 S3/MinIO 或 Local FS 实现。
2.  **向量库轻量化**：集成 `pymilvus` 的 Lite 模式，或在 `docker-compose.yml` 中配置单机版 Milvus。

### 5.3 阶段三：部署配置 (Deployment)

1.  **编写 Docker Compose**：
    创建一个 `docker-compose.yml` 文件，包含以下服务：
    *   `backend`: Flask 应用
    *   `celery_worker`: 异步任务处理
    *   `frontend`: Nginx + React 静态文件
    *   `redis`: 消息队列
    *   `elasticsearch`: 单节点
    *   `nebula`: 单机版图数据库
    
2.  **资源限制**：在 Compose 文件中为每个容器设置 `mem_limit`，防止单机内存溢出。

## 6. 单机性能优化策略

1.  **模型加载优化**：
    *   **延迟加载 (Lazy Loading)**：NLP 模型（如 Embedding, NER）仅在 Celery Worker 中加载，Flask Web 进程不加载重型模型，减少内存占用。
    *   **共享内存**：如果使用 Gunicorn 多 Worker 模式，利用 Linux `fork` 机制尽可能共享只读模型数据。

2.  **任务队列调优**：
    *   控制 Celery 的并发度 (`--concurrency`)，避免开启过多进程导致 CPU 频繁切换。建议设置为 CPU 核心数 - 1。

3.  **数据库调优**：
    *   **ES**：减少分片数（`number_of_shards: 1`），关闭副本（`number_of_replicas: 0`）。
    *   **Nebula**：调整 buffer pool 大小以适应单机内存。

## 7. 总结

通过上述改造，我们将原本复杂的分布式微服务架构简化为**“单体应用 + 本地服务组件”**的模式。这种架构完全保留了 GraphRAG 的核心能力（知识抽取、混合检索、图谱推理），但部署仅需运行一个 `docker-compose up` 命令，极大降低了项目的交付门槛和运维成本，非常适合中小规模企业内部部署或个人知识库场景。
