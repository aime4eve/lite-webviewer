# 知识图谱全文检索单体服务改造方案

## 1. 概述

本文档旨在将原定的基于微服务架构的“知识图谱全文检索系统”改造为适合单台服务器部署的**单体服务架构**。改造的核心目标是降低运维复杂度、减少资源消耗，同时保持原方案中关于 GraphRAG（图谱增强检索）的核心技术实现不变。

## 2. 架构改造分析

### 2.1 原微服务架构梳理

原方案采用典型的微服务分层架构：
*   **应用服务层**：包含 SearchAPI、KGAPI、AdminAPI 三个独立服务，通过 API Gateway 路由。
*   **知识处理层**：包含 DocIngest、NLPProcess、KGBuilder、VectorEncoder 四个独立 Worker，通过 Redis 消息队列异步通信。
*   **数据存储层**：依赖 MinIO、Elasticsearch、Milvus、NebulaGraph、Redis 等多个分布式组件。

**痛点分析（针对单机部署）**：
1.  **组件过多**：运维管理复杂，Docker 容器编排繁琐。
2.  **通信开销**：服务间 HTTP/RPC 调用和序列化带来不必要的延迟。
3.  **资源浪费**：每个微服务独立的运行环境（Python 解释器、Web Server 进程）占用大量内存。

### 2.2 单体架构设计理念

*   **进程内通信替代网络通信**：将原来的 HTTP 调用改为 Python 模块间的函数调用。
*   **共享运行时**：所有业务逻辑运行在同一个 Flask 应用进程或 Celery Worker 进程中。
*   **简化中间件**：移除 API Gateway，统一由 Nginx 做反向代理；简化存储组件部署方式。

## 3. 单体服务架构设计

### 3.1 系统架构图

```mermaid
graph TD
    User[用户] --> WebUI[React 前端应用]
    WebUI --> Nginx[Nginx 反向代理]
    
    subgraph "单体后端服务 (Monolithic Backend)"
        Nginx --> FlaskApp[Flask 主应用进程]
        
        subgraph "模块划分 (Python Modules)"
            FlaskApp --> APIBlueprint["API 蓝图 (Search/KG/Admin)"]
            APIBlueprint --> ServiceLayer[业务服务层]
            
            ServiceLayer --> SearchEngine[检索引擎模块]
            ServiceLayer --> KGEngine[图谱引擎模块]
            ServiceLayer --> DocManager[文档管理模块]
            
            SearchEngine --> RAGCore[RAG 核心逻辑]
        end
        
        ServiceLayer -.-> AsyncTasks["异步任务队列 (Celery)"]
    end
    
    subgraph "异步处理进程 (Celery Worker)"
        AsyncTasks --> TaskWorker[通用任务 Worker]
        TaskWorker --> Pipeline[处理流水线]
        
        Pipeline --> P_Extract[文本提取]
        Pipeline --> P_Vector[向量编码]
        Pipeline --> P_NLP[NLP 处理]
        Pipeline --> P_Graph[图谱构建]
    end
    
    subgraph "本地数据存储 (Docker Compose)"
        FlaskApp & TaskWorker --> LocalFS["本地文件存储 (替代 MinIO)"]
        FlaskApp & TaskWorker --> ES_Single["Elasticsearch (单节点)"]
        FlaskApp & TaskWorker --> Milvus_Lite["Milvus Lite / FAISS"]
        FlaskApp & TaskWorker --> Nebula_Single["NebulaGraph (单机版)"]
        FlaskApp & TaskWorker --> Redis["Redis (缓存+队列)"]
        FlaskApp & TaskWorker --> SQLite["SQLite/MySQL (元数据)"]
    end

```

### 3.2 模块合并策略

1.  **API 层合并**：
    *   取消独立的 SearchAPI、KGAPI、AdminAPI 服务。
    *   使用 Flask Blueprint 将它们合并为同一个 Flask 应用的不同路由模块：
        *   `/api/search/*` -> `search_blueprint`
        *   `/api/kg/*` -> `kg_blueprint`
        *   `/api/admin/*` -> `admin_blueprint`

2.  **业务逻辑层合并**：
    *   原微服务中的业务逻辑代码重构为 Python Package 下的不同模块（`services.search`, `services.kg`, `services.ingest`）。
    *   模块间调用直接通过 `import` 和函数调用实现，移除 HTTP 请求代码。

3.  **异步任务合并**：
    *   原 DocIngest、NLPProcess 等独立 Worker 合并为一个通用的 Celery Worker 服务。
    *   通过 Celery 的 Task 路由机制区分不同类型的任务，但运行在同一个进程池中，共享内存资源（如加载一次 NLP 模型）。

## 4. 核心技术与实现细节 (Core Technologies & Implementation)

本章节详细补充了在单体架构下，如何实现和优化原方案（参考 `知识图谱的全文检索技术实施方案.md`）中的核心技术点。

### 4.1 全文检索核心算法实现

在单体架构中，Elasticsearch (ES) 仍作为全文检索的核心组件，但部署模式调整为单节点。

#### 4.1.1 倒排索引机制 (Inverted Index)
*   **原理**：将文档中的内容分词后，建立“词项 (Term) -> 文档ID (DocID)”的映射关系，支持毫秒级关键词查询。
*   **单体适配**：
    *   **索引结构**：采用 `_source` 存储原始文本块，利用 `keyword` 类型存储不分词的元数据（如文件路径、创建时间）。
    *   **分段管理**：控制 Segment 合并策略，避免小段过多占用文件句柄（单机资源有限）。

#### 4.1.2 中文分词算法 (Tokenization)
*   **组件**：集成 `ik_max_word`（细粒度切分）和 `ik_smart`（粗粒度切分）插件。
*   **自定义词库**：
    *   **动态加载**：在单体服务启动时，从本地 `dict/` 目录加载行业专有词汇（如“NebulaGraph”、“知识图谱”）。
    *   **实现**：通过 ES 的 `remote_ext_dict` 配置指向本地 Nginx 托管的词库文件，实现热更新。

#### 4.1.3 相关性评分 (Relevance Scoring)
*   **算法**：默认采用 **BM25** 算法。
    *   $Score(D,Q) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}$
*   **优化策略**：
    *   **混合打分 (Function Score)**：结合文档的时效性（`publish_date`）和重要性权重（`weight` 字段）进行加权。
    *   **实施**：在 `SearchEngine` 模块中封装 DSL 构建器，自动注入权重参数。

### 4.2 关键技术组件集成

在单体架构下，各组件通过进程内调用或本地 Socket 通信，减少网络开销。

#### 4.2.1 索引引擎 (Indexing Engine)
*   **职责**：负责文档的解析、分块、向量化及多路存储。
*   **流程**：
    1.  **Text Splitter**：基于 `LangChain` 的 `RecursiveCharacterTextSplitter`，按语义完整性切分文本（Chunk Size: 512 tokens）。
    2.  **Vector Encoder**：调用本地加载的 `BGE-M3` 模型（运行在 Celery Worker 进程中），生成 1024 维向量。
    3.  **Storage Router**：
        *   -> ES: 存文本 + 关键词索引
        *   -> Milvus Lite: 存向量 + ChunkID
        *   -> NebulaGraph: 存实体关系 + ChunkID 关联

#### 4.2.2 查询处理器 (Query Processor)
*   **职责**：统一处理用户查询，执行 Intent Recognition 和 Query Rewriting。
*   **实现**：
    *   **Intent Classifier**：使用轻量级 BERT 模型（或规则引擎）判断查询意图（事实型/关系型/探索型）。
    *   **Graph Traversal**：对于关系型查询，生成 NebulaGraph 的 nGQL 语句（如 `MATCH (v:Entity)-[e:RELATION*1..2]->(v2) WHERE id(v) == 'EntityA' RETURN path`）。

#### 4.2.3 缓存机制 (Caching Strategy)
*   **多级缓存**：
    *   **L1 (Local Memory)**：使用 `functools.lru_cache` 缓存高频的配置项和元数据（如 Schema 定义）。
    *   **L2 (Redis)**：缓存复杂的查询结果（Query Hash -> Result JSON），TTL 设置为 10 分钟。
*   **实施**：在 `ServiceLayer` 中使用 Python 装饰器 `@cache_result(ttl=600)` 自动管理 Redis 缓存。

## 5. 基础设施与存储适配

针对单机环境，对基础设施进行轻量化调整：

### 5.1 存储组件调整

| 组件 | 原方案 | 单体/单机方案 | 改造说明 |
| :--- | :--- | :--- | :--- |
| **文件存储** | MinIO Cluster | **Local Filesystem / MinIO Single** | 优先使用本地文件系统存储上传文档，或部署单节点 MinIO 以保持接口一致性。 |
| **向量库** | Milvus Cluster | **Milvus Lite / FAISS** | 对于中小规模数据（<100万向量），使用嵌入式的 Milvus Lite 或直接集成 FAISS 库，无需独立部署服务。 |
| **全文检索** | Elasticsearch Cluster | **Elasticsearch Single Node** | 部署单节点 ES，限制堆内存（如 2GB），关闭副本分片。 |
| **图数据库** | NebulaGraph Cluster | **NebulaGraph Docker (Standalone)** | 使用 NebulaGraph 的单机 Docker 镜像，包含 Graphd/Metad/Storaged 所有服务。 |
| **元数据** | PostgreSQL/MySQL | **SQLite / PostgreSQL Docker** | 推荐使用 SQLite 减少维护成本，或单容器 PostgreSQL。 |

### 5.2 服务发现与网关

*   **移除 API Gateway**：不再需要 Kong 或 Spring Cloud Gateway。
*   **Nginx 代理**：在单机上部署一个 Nginx，负责：
    *   托管 React 前端静态资源。
    *   反向代理 `/api` 请求到本地 Flask 端口（如 5000）。
    *   SSL 终结（如果需要）。

## 6. 实施步骤

### 6.1 阶段一：代码重构 (Refactoring)

1.  **建立单体工程结构**：
    ```text
    /src
      /backend
        /app
          /api          # 存放所有 Blueprint (Search, KG, Admin)
          /services     # 核心业务逻辑 (原微服务核心代码)
          /models       # 数据库模型
          /tasks        # Celery 任务定义
          /utils        # 工具类
        config.py       # 统一配置文件
        main.py         # Flask 入口
    ```
2.  **合并 API 接口**：将原分散的 Swagger/OpenAPI 定义合并，统一注册到 Flask App。
3.  **内联服务调用**：查找所有 `requests.post('http://other-service/...')` 代码，替换为 `from services.other import func; func(...)`。

### 6.2 阶段二：存储层适配 (Storage Adaptation)

1.  **本地化文件适配器**：编写 `FileManager` 接口，支持切换 S3/MinIO 或 Local FS 实现。
2.  **向量库轻量化**：集成 `pymilvus` 的 Lite 模式，或在 `docker-compose.yml` 中配置单机版 Milvus。

### 6.3 阶段三：部署配置 (Deployment)

1.  **编写 Docker Compose**：
    创建一个 `docker-compose.yml` 文件，包含以下服务：
    *   `backend`: Flask 应用
    *   `celery_worker`: 异步任务处理
    *   `frontend`: Nginx + React 静态文件
    *   `redis`: 消息队列
    *   `elasticsearch`: 单节点
    *   `nebula`: 单机版图数据库
    
2.  **资源限制**：在 Compose 文件中为每个容器设置 `mem_limit`，防止单机内存溢出。

## 7. 单机性能优化策略

### 7.1 索引与查询优化

*   **Elasticsearch 优化**：
    *   **Shard 设置**：单机环境下设置 `number_of_shards: 1`，避免多分片带来的 Lucene 归并开销。
    *   **Refresh Interval**：批量导入数据时，将 `refresh_interval` 调整为 `-1` 或 `30s`，提升写入吞吐量。
*   **NebulaGraph 优化**：
    *   **Space 创建**：`CREATE SPACE` 时设置 `replica_factor = 1` 和 `partition_num = 5`（适应单机 CPU 核数）。
    *   **查询剪枝**：在 nGQL 中限制 `LIMIT` 和 `MAX_DEPTH`（如最大 3 跳），防止深层遍历耗尽内存。

### 7.2 资源管理与并发控制

*   **模型加载优化 (Model Loading)**：
    *   **延迟加载 (Lazy Loading)**：NLP 模型（如 Embedding, NER）仅在 Celery Worker 进程初始化时加载，Flask Web 进程不加载重型模型，减少内存占用。
    *   **共享内存 (Shared Memory)**：使用 Linux `fork` 机制（Celery `prefork` 模式），父进程预加载模型，子进程通过 Copy-on-Write 共享内存。
*   **并发控制 (Concurrency Control)**：
    *   **Celery**：设置 `worker_concurrency` 为 `CPU核心数 - 1`，预留 1 核给数据库和系统进程。
    *   **Flask**：使用 Gevent 或 Gunicorn 异步 Worker 处理 I/O 密集型请求。

## 8. 总结

通过上述改造，我们将原本复杂的分布式微服务架构简化为**“单体应用 + 本地服务组件”**的模式。这种架构完全保留了 GraphRAG 的核心能力（知识抽取、混合检索、图谱推理），但部署仅需运行一个 `docker-compose up` 命令，极大降低了项目的交付门槛和运维成本，非常适合中小规模企业内部部署或个人知识库场景。
